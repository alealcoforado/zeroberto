{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hdbscan\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = dataset['train'].select(range(0,100))\n",
    "st = SentenceTransformer('all-roberta-large-v1')\n",
    "embeddings = st.encode(train_dataset['text'])\n",
    "clusterer = hdbscan.HDBSCAN(leaf_size=10, min_cluster_size=2)\n",
    "clusters = clusterer.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs = np.array([(np.arange(0,100,1) * np.arange(100,0,-1) / 1000), (np.arange(0,100,1) * np.arange(100,0,-1) / 2000),\n",
    "         (np.arange(0,100,1) * np.arange(100,0,-1) / 2000),(np.arange(0,100,1) * 2.7*np.arange(100,0,-1) / 2000)]).reshape(100,4)\n",
    "labels = dataset['train'].select(range(0,100))['label']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def _get_intraclass_clustering_data(text_list,probabilities,true_labels,embeddings,n,clusterer='hdbscan',leaf_size=10,min_cluster_size=10):\n",
    "    df_probs = pd.DataFrame(probabilities)\n",
    "    label_results = df_probs.apply(lambda row: row.idxmax(), axis=1).to_list()\n",
    "    prob_results = df_probs.apply(lambda row: row.max(), axis=1).to_list()\n",
    "\n",
    "    unique_labels = list(set(label_results))\n",
    "    unique_labels.sort()\n",
    "    print(unique_labels)\n",
    "    all_labels_selected_data = []\n",
    "    for label in unique_labels:\n",
    "        this_label_selected_data = []\n",
    "        this_label_indexes = [i for i in range(len(label_results)) if label_results[i] == label]\n",
    "        # print(this_label_indexes)\n",
    "        this_label_text_list =  [text_list[i] for i in this_label_indexes]\n",
    "        this_label_embeddings =  [embeddings[i] for i in this_label_indexes]\n",
    "        this_label_probs =  [prob_results[i] for i in this_label_indexes]\n",
    "        this_label_true_labels = [true_labels[i] for i in this_label_indexes]\n",
    "        this_label_label_results = [label_results[i] for i in this_label_indexes]\n",
    "\n",
    "\n",
    "        print(\"Clustering class {}.\".format(label))\n",
    "        # logger.info(\"Clustering class {}.\")\n",
    "\n",
    "        this_label_clusters = _clusterer_fit_predict(clusterer, this_label_embeddings, leaf_size, min_cluster_size) \n",
    "        # print(len(this_label_clusters),len(this_label_indexes),len(this_label_text_list),len(this_label_embeddings),len(this_label_probs))\n",
    "        unique_clusters = list(set(this_label_clusters))\n",
    "        unique_clusters.sort()\n",
    "        # print(unique_clusters)\n",
    "        all_clusters_sorted_lists = []\n",
    "\n",
    "        # organize by sorting sorting and zipping lists, 1 list for each cluster found\n",
    "        for cluster in unique_clusters:\n",
    "            this_cluster_indexes = [i for i in range(len(this_label_clusters)) if this_label_clusters[i] == cluster]\n",
    "            this_cluster_probs =  [this_label_probs[i] for i in this_cluster_indexes]\n",
    "            this_cluster_texts = [this_label_text_list[i] for i in this_cluster_indexes]\n",
    "            this_cluster_true_labels = [this_label_true_labels[i] for i in this_cluster_indexes]\n",
    "            this_cluster_label_results = [this_label_label_results[i] for i in this_cluster_indexes]\n",
    "            zipped_lists = (list(zip(this_cluster_probs,this_cluster_indexes,this_cluster_true_labels,this_cluster_label_results,this_cluster_texts)))\n",
    "            zipped_lists.sort(reverse=True)\n",
    "            # print(zipped_lists)\n",
    "\n",
    "            all_clusters_sorted_lists.append(zipped_lists)\n",
    "        # selects data iteratively, 1 from each cluster from biggest to smallest cluster, \n",
    "        # following highest probability order inside each cluster\n",
    "        while len(this_label_selected_data) < n:\n",
    "            for sorted_list in all_clusters_sorted_lists:\n",
    "                # print((all_clusters_sorted_lists))\n",
    "                if len(sorted_list) > 0:\n",
    "                    # print(sorted_list)\n",
    "                    selected_element = sorted_list[0]\n",
    "                    print(label,selected_element)\n",
    "                    this_label_selected_data.append(selected_element)\n",
    "                    sorted_list.pop(0)\n",
    "                    # print(sorted_list)\n",
    "                    if len(this_label_selected_data) == n:\n",
    "                        break\n",
    "\n",
    "                # print(all_clusters_sorted_lists)\n",
    "            if len(all_clusters_sorted_lists) == 0 or all_clusters_sorted_lists == [[]]:\n",
    "                # print(label)\n",
    "                print(\"Not enough data to sample for label {label}: {n} samples expected, but only got {this_label_n}\".format(label=label,n=n,this_label_n=len(this_label_selected_data)))\n",
    "                # logger.info(\"Not enough data to sample for label {label}: {n} samples expected, but only got {this_label_n}\".format(label=label,n=n,this_label_n=len(this_label_selected_data)))\n",
    "                break\n",
    "        all_labels_selected_data.append(this_label_selected_data)\n",
    "\n",
    "    flat_selected_data = [item for sublist in all_labels_selected_data for item in sublist]\n",
    "\n",
    "    probs,train_indices,true_labels,train_labels,texts = zip(*flat_selected_data)\n",
    "\n",
    "\n",
    "    # x_train = [text_list[i] for i in train_indices]\n",
    "    # y_train = [true_labels[i] for i in train_indices]\n",
    "    # labels_train = [label_results[i] for i in train_indices]\n",
    "    # print(x_train,y_train,labels_train)\n",
    "\n",
    "    x_train = texts\n",
    "    y_train = true_labels\n",
    "    labels_train = train_labels \n",
    "    print(x_train,y_train,labels_train)\n",
    "                                 \n",
    "    return x_train, y_train, labels_train\n",
    "\n",
    "def _clusterer_fit_predict(clusterer,embeddings,leaf_size,min_cluster_size):\n",
    "    if clusterer=='hdbscan':\n",
    "        clusterer = hdbscan.HDBSCAN(leaf_size=leaf_size, min_cluster_size=min_cluster_size)\n",
    "    # print(len(embeddings))\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "    # logger.info(\"Found {} clusters.\".format(len(list(set(clusters)))))\n",
    "    print(\"Found {} clusters.\".format(len(list(set(clusters)))))\n",
    "    return clusters\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, labels_train = _get_intraclass_clustering_data(train_dataset['text'],probs,labels,embeddings,8,min_cluster_size=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
