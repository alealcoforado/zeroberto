{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hdbscan\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import collections\n",
    "from torch import tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([9, 6])\n"
     ]
    }
   ],
   "source": [
    "# discard_indices = [tensor(2, device='cpu')]\n",
    "discard_indices = [torch.tensor(2,device='mps'),torch.tensor(3),torch.tensor(4)]\n",
    "discard_indices = torch.tensor(discard_indices,dtype=torch.float64)\n",
    "probs = [[0.33,0.44,0.55,0.66,0.31,0.98], [0.11,0.44,0.55,0.66,0.31,0.98], [0.22,0.54,0.32,0.451,0.32,0.98], [0.11,0.44,0.55,0.66,0.31,0.98], [0.22,0.54,0.32,0.451,0.32,0.98], [0.11,0.44,0.55,0.66,0.31,0.98], [0.22,0.54,0.32,0.451,0.32,0.98], [0.11,0.44,0.55,0.66,0.31,0.98], [0.22,0.54,0.32,0.451,0.32,0.98]]\n",
    "probs = torch.Tensor(probs)\n",
    "\n",
    "print(type(probs),(probs.shape))\n",
    "\n",
    "tensor = torch.ones(len(discard_indices),probs.shape[-1])\n",
    "probs[(discard_indices)]\n",
    "\n",
    "for prob in probs:\n",
    "    probs\n",
    "# print(probs)\n",
    "# v,i = torch.topk(probs,k=1,dim=0)\n",
    "# top_prob,index = v.T,i.T\n",
    "# top_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[discard_indices] = -1.0*torch.ones(len(discard_indices), probs.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "dict = {'eval':{'weighted':{'accuracy':2}}}\n",
    "print(dict[list(dict.keys())[0]]['weighted']['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/alealcoforado/.cache/huggingface/datasets/SetFit___json/SetFit--CR-55cf29835d8d8adf/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 2/2 [00:00<00:00, 557.27it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cr_ds \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39mSetFit/CR\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m cr_ds[\u001b[39m'\u001b[39;49m\u001b[39mlabel_text\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/datasets/dataset_dict.py:57\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, k) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dataset:\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, (\u001b[39mstr\u001b[39m, NamedSplit)) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(k)\n\u001b[1;32m     58\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m         available_suggested_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     60\u001b[0m             split \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m (Split\u001b[39m.\u001b[39mTRAIN, Split\u001b[39m.\u001b[39mTEST, Split\u001b[39m.\u001b[39mVALIDATION) \u001b[39mif\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m     61\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label_text'"
     ]
    }
   ],
   "source": [
    "cr_ds = load_dataset('SetFit/CR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'negative']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_ds['train']['label_text'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 5, 15, 6], [2, 1, 2, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix = [[0,1,2],[3,5,4],[10,13,15],[6,5,4]]    \n",
    "df_test = pd.DataFrame(test_matrix)\n",
    "idxmaxes = df_test.apply(lambda row: row.idxmax(), axis=1).to_list()\n",
    "maxes = df_test.apply(lambda row: row.max(), axis=1).to_list()\n",
    "\n",
    "maxes,idxmaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 5, 15, 6], [2, 1, 2, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_max = [np.argmax(lista) for lista in (np.array(test_matrix))]\n",
    "valores_max = [np.max(lista) for lista in (np.array(test_matrix))]\n",
    "\n",
    "valores_max,indices_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 2, 3] at entry 0 and [4, 5, 6] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m array \u001b[39min\u001b[39;00m test_arrays:\n\u001b[1;32m      4\u001b[0m     new\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mTensor(array))\n\u001b[0;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mstack(new))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 2, 3] at entry 0 and [4, 5, 6] at entry 1"
     ]
    }
   ],
   "source": [
    "test_arrays = ([np.ndarray([1,2,3]),np.ndarray([4,5,6]),np.ndarray([4,5,6]),np.ndarray([4,5,6])])\n",
    "new = []\n",
    "for array in test_arrays:\n",
    "    new.append(torch.Tensor(array))\n",
    "print(torch.stack(new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m test_tensors  \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mTensor([\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m]),torch\u001b[39m.\u001b[39mTensor([\u001b[39m3\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m4\u001b[39m]),torch\u001b[39m.\u001b[39mTensor([\u001b[39m10\u001b[39m,\u001b[39m13\u001b[39m,\u001b[39m15\u001b[39m]),torch\u001b[39m.\u001b[39mTensor([\u001b[39m6\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m4\u001b[39m])]    \n\u001b[1;32m      3\u001b[0m \u001b[39m# torch.stack(test_tensors)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m torch\u001b[39m.\u001b[39;49mTensor(test_tensors)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "test_tensors  = [torch.Tensor([0,1,2]),torch.Tensor([3,5,4]),torch.Tensor([10,13,15]),torch.Tensor([6,5,4])]    \n",
    "\n",
    "# torch.stack(test_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed in embeddings:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/Users/alealcoforado/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|██████████| 2/2 [00:00<00:00, 59.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mag_news\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mselect(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m100\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m st \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39;49m\u001b[39mall-roberta-large-v1\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m embeddings \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39mencode(train_dataset[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m clusterer \u001b[39m=\u001b[39m hdbscan\u001b[39m.\u001b[39mHDBSCAN(leaf_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, min_cluster_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[1;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[1;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[1;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[1;32m    839\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 840\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m    841\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[1;32m    843\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(sbert_config_path) \u001b[39mas\u001b[39;00m fIn:\n\u001b[1;32m    136\u001b[0m     config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[0;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m Transformer(model_name_or_path\u001b[39m=\u001b[39;49minput_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:29\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[1;32m     28\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[0;32m---> 29\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[39mif\u001b[39;00m tokenizer_name_or_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model_name_or_path, cache_dir\u001b[39m=\u001b[39mcache_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_args)\n\u001b[1;32m     33\u001b[0m \u001b[39m#No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:49\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path, config\u001b[39m=\u001b[39;49mconfig, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    470\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    473\u001b[0m     )\n\u001b[1;32m    474\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/transformers/modeling_utils.py:2795\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2786\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2788\u001b[0m     (\n\u001b[1;32m   2789\u001b[0m         model,\n\u001b[1;32m   2790\u001b[0m         missing_keys,\n\u001b[1;32m   2791\u001b[0m         unexpected_keys,\n\u001b[1;32m   2792\u001b[0m         mismatched_keys,\n\u001b[1;32m   2793\u001b[0m         offload_index,\n\u001b[1;32m   2794\u001b[0m         error_msgs,\n\u001b[0;32m-> 2795\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2796\u001b[0m         model,\n\u001b[1;32m   2797\u001b[0m         state_dict,\n\u001b[1;32m   2798\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2799\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2800\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2801\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2802\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2803\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2804\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2805\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2806\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2807\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2808\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2809\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49mload_in_8bit,\n\u001b[1;32m   2810\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   2811\u001b[0m     )\n\u001b[1;32m   2813\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n\u001b[1;32m   2815\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/transformers/modeling_utils.py:3077\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3068\u001b[0m     \u001b[39m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   3069\u001b[0m     mismatched_keys \u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3070\u001b[0m         state_dict,\n\u001b[1;32m   3071\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3075\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   3076\u001b[0m     )\n\u001b[0;32m-> 3077\u001b[0m     error_msgs \u001b[39m=\u001b[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[1;32m   3078\u001b[0m     offload_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3079\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   3081\u001b[0m \n\u001b[1;32m   3082\u001b[0m     \u001b[39m# This should always be a list but, just to be sure.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/transformers/modeling_utils.py:529\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m             load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 529\u001b[0m load(model_to_load, state_dict, prefix\u001b[39m=\u001b[39;49mstart_prefix)\n\u001b[1;32m    530\u001b[0m \u001b[39m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/transformers/modeling_utils.py:527\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/transformers/modeling_utils.py:527\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 527 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/transformers/modeling_utils.py:527\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/transformers/modeling_utils.py:523\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    521\u001b[0m                     module\u001b[39m.\u001b[39m_load_from_state_dict(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    522\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         module\u001b[39m.\u001b[39;49m_load_from_state_dict(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv_zeroberto/lib/python3.11/site-packages/torch/nn/modules/module.py:1942\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   1940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1941\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1942\u001b[0m         param\u001b[39m.\u001b[39;49mcopy_(input_param)\n\u001b[1;32m   1943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m   1944\u001b[0m     error_msgs\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1945\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the model are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1946\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the checkpoint are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1947\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39man exception occurred : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1948\u001b[0m                       \u001b[39m.\u001b[39mformat(key, param\u001b[39m.\u001b[39msize(), input_param\u001b[39m.\u001b[39msize(), ex\u001b[39m.\u001b[39margs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = dataset['train'].select(range(0,100))\n",
    "st = SentenceTransformer('all-roberta-large-v1')\n",
    "embeddings = st.encode(train_dataset['text'])\n",
    "clusterer = hdbscan.HDBSCAN(leaf_size=10, min_cluster_size=2)\n",
    "clusters = clusterer.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs = np.array([(np.arange(0,100,1) * np.arange(100,0,-1) / 1000), (np.arange(0,100,1) * np.arange(100,0,-1) / 2000),\n",
    "         (np.arange(0,100,1) * np.arange(100,0,-1) / 2000),(np.arange(0,100,1) * 2.7*np.arange(100,0,-1) / 2000)]).reshape(100,4)\n",
    "labels = dataset['train'].select(range(0,100))['label']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_intraclass_clustering_data(text_list,probabilities,true_labels,embeddings,n,clusterer='hdbscan',leaf_size=10,min_cluster_size=10):\n",
    "    df_probs = pd.DataFrame(probabilities)\n",
    "    label_results = df_probs.apply(lambda row: row.idxmax(), axis=1).to_list()\n",
    "    prob_results = df_probs.apply(lambda row: row.max(), axis=1).to_list()\n",
    "\n",
    "    unique_labels = list(set(label_results))\n",
    "    unique_labels.sort()\n",
    "    print(unique_labels)\n",
    "    all_labels_selected_data = []\n",
    "    for label in unique_labels:\n",
    "        this_label_selected_data = []\n",
    "        this_label_indexes = [i for i in range(len(label_results)) if label_results[i] == label]\n",
    "        # print(this_label_indexes)\n",
    "        this_label_text_list =  [text_list[i] for i in this_label_indexes]\n",
    "        this_label_embeddings =  [embeddings[i] for i in this_label_indexes]\n",
    "        this_label_probs =  [prob_results[i] for i in this_label_indexes]\n",
    "        this_label_true_labels = [true_labels[i] for i in this_label_indexes]\n",
    "        this_label_label_results = [label_results[i] for i in this_label_indexes]\n",
    "\n",
    "\n",
    "        print(\"Clustering class {}.\".format(label))\n",
    "        # logger.info(\"Clustering class {}.\")\n",
    "\n",
    "        this_label_clusters = _clusterer_fit_predict(clusterer, this_label_embeddings, leaf_size, min_cluster_size) \n",
    "        # print(len(this_label_clusters),len(this_label_indexes),len(this_label_text_list),len(this_label_embeddings),len(this_label_probs))\n",
    "        unique_clusters = list(set(this_label_clusters))\n",
    "        unique_clusters.sort()\n",
    "        # print(unique_clusters)\n",
    "        all_clusters_sorted_lists = []\n",
    "\n",
    "        # organize by sorting sorting and zipping lists, 1 list for each cluster found\n",
    "        for cluster in unique_clusters:\n",
    "            this_cluster_indexes = [i for i in range(len(this_label_clusters)) if this_label_clusters[i] == cluster]\n",
    "            this_cluster_probs =  [this_label_probs[i] for i in this_cluster_indexes]\n",
    "            this_cluster_texts = [this_label_text_list[i] for i in this_cluster_indexes]\n",
    "            this_cluster_true_labels = [this_label_true_labels[i] for i in this_cluster_indexes]\n",
    "            this_cluster_label_results = [this_label_label_results[i] for i in this_cluster_indexes]\n",
    "            zipped_lists = (list(zip(this_cluster_probs,this_cluster_indexes,this_cluster_true_labels,this_cluster_label_results,this_cluster_texts)))\n",
    "            zipped_lists.sort(reverse=True)\n",
    "            # print(zipped_lists)\n",
    "\n",
    "            all_clusters_sorted_lists.append(zipped_lists)\n",
    "        # selects data iteratively, 1 from each cluster from biggest to smallest cluster, \n",
    "        # following highest probability order inside each cluster\n",
    "        while len(this_label_selected_data) < n:\n",
    "            for sorted_list in all_clusters_sorted_lists:\n",
    "                # print((all_clusters_sorted_lists))\n",
    "                if len(sorted_list) > 0:\n",
    "                    # print(sorted_list)\n",
    "                    selected_element = sorted_list[0]\n",
    "                    print(label,selected_element)\n",
    "                    this_label_selected_data.append(selected_element)\n",
    "                    sorted_list.pop(0)\n",
    "                    # print(sorted_list)\n",
    "                    if len(this_label_selected_data) == n:\n",
    "                        break\n",
    "\n",
    "                # print(all_clusters_sorted_lists)\n",
    "            if len(all_clusters_sorted_lists) == 0 or all_clusters_sorted_lists == [[]]:\n",
    "                # print(label)\n",
    "                print(\"Not enough data to sample for label {label}: {n} samples expected, but only got {this_label_n}\".format(label=label,n=n,this_label_n=len(this_label_selected_data)))\n",
    "                # logger.info(\"Not enough data to sample for label {label}: {n} samples expected, but only got {this_label_n}\".format(label=label,n=n,this_label_n=len(this_label_selected_data)))\n",
    "                break\n",
    "        all_labels_selected_data.append(this_label_selected_data)\n",
    "\n",
    "    flat_selected_data = [item for sublist in all_labels_selected_data for item in sublist]\n",
    "\n",
    "    probs,train_indices,true_labels,train_labels,texts = zip(*flat_selected_data)\n",
    "\n",
    "\n",
    "    # x_train = [text_list[i] for i in train_indices]\n",
    "    # y_train = [true_labels[i] for i in train_indices]\n",
    "    # labels_train = [label_results[i] for i in train_indices]\n",
    "    # print(x_train,y_train,labels_train)\n",
    "\n",
    "    x_train = texts\n",
    "    y_train = true_labels\n",
    "    labels_train = train_labels \n",
    "    print(x_train,y_train,labels_train)\n",
    "                                 \n",
    "    return x_train, y_train, labels_train\n",
    "\n",
    "def _clusterer_fit_predict(clusterer,embeddings,leaf_size,min_cluster_size):\n",
    "    if clusterer=='hdbscan':\n",
    "        clusterer = hdbscan.HDBSCAN(leaf_size=leaf_size, min_cluster_size=min_cluster_size)\n",
    "    # print(len(embeddings))\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "    # logger.info(\"Found {} clusters.\".format(len(list(set(clusters)))))\n",
    "    print(\"Found {} clusters.\".format(len(list(set(clusters)))))\n",
    "    return clusters\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, labels_train = _get_intraclass_clustering_data(train_dataset['text'],probs,labels,embeddings,8,min_cluster_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_zeroberto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
