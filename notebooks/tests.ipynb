{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/alealcoforado/Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(['tn','ic']*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python history_correlation.py --input_dir=\"/Users/alealcoforado/Documents/resultados\" --files_start dbpedia_14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/alealcoforado/.cache/huggingface/datasets/SetFit___json/SetFit--ag_news-bfd04d52440715fd/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 2/2 [00:00<00:00, 328.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset('SetFit/ag_news')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data['train'].shuffle(seed=333).select(range(5000))\n",
    "test_dataset = data['test'][0:1000]\n",
    "classes_list=['sports','business','science','world']\n",
    "hypothesis_template = 'This text is {}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This text is {}.' 'This text is {}.' 'This text is {}.'\n",
      " 'This text is {}.']\n"
     ]
    }
   ],
   "source": [
    "from ZeroBERTo import trainer, modeling_zeroberto\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "print(np.repeat([hypothesis_template],len(classes_list)))\n",
    "def build_first_shot_dataset():\n",
    "    x_train = [hypothesis_template.format(cl) for cl in classes_list]\n",
    "    y_train = np.arange(len(classes_list))\n",
    "    return x_train, y_train\n",
    "    print(x_train,y_train)\n",
    "    pass\n",
    "# build_first_shot_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroBERTo Body device: mps\n",
      "1st Shot Model Device: mps\n"
     ]
    }
   ],
   "source": [
    "model = modeling_zeroberto.ZeroBERToModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                            hypothesis_template=hypothesis_template,\n",
    "                                            classes_list=classes_list,\n",
    "                                            )\n",
    "# Set up Data Selector\n",
    "data_selector = modeling_zeroberto.ZeroBERToDataSelector()\n",
    "\n",
    "# Build trainer\n",
    "t = trainer.ZeroBERToTrainer(\n",
    "    model=model,\n",
    "    # data_selector=data_selector,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_probs, embeds, original_logits = t.model.first_shot_model(train_dataset[\"text\"], return_embeddings=True, return_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import torch\n",
    "import time\n",
    "def _clusterer_fit_predict(clusterer,embeddings,leaf_size,min_cluster_size):\n",
    "    t0 = time.time()\n",
    "    if clusterer=='hdbscan':\n",
    "        clusterer_model = hdbscan.HDBSCAN(leaf_size=leaf_size, min_cluster_size=min_cluster_size)\n",
    "    embeds = torch.Tensor(embeddings).cpu()\n",
    "    clusters = clusterer_model.fit_predict(embeds)\n",
    "    # logger.info(\"Found {} clusters.\".format(len(list(set(clusters)))))\n",
    "    num_of_clusters = len(list(set(clusters)))\n",
    "    if num_of_clusters!= 1:\n",
    "        keep_training = True\n",
    "    print(f\"Found {num_of_clusters} clusters.\",f\" --- Time: {time.time()-t0}\")\n",
    "    print()\n",
    "    return torch.IntTensor(clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 clusters.  --- Time: 14.306386232376099\n",
      "\n",
      "Found 3 clusters.  --- Time: 14.702558994293213\n",
      "\n",
      "Found 3 clusters.  --- Time: 13.880327939987183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clusters = _clusterer_fit_predict('hdbscan',embeds,10,10)\n",
    "clusters = _clusterer_fit_predict('hdbscan',embeds,10,50)\n",
    "clusters = _clusterer_fit_predict('hdbscan',embeds,10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6]\n",
    "a[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = trainer.ZeroBERToTrainer(SentenceTransformer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hdbscan\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import collections\n",
    "from torch import tensor\n",
    "\n",
    "# x_train,y_train,probs,embeds,indice = (zip(*lista))\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modeling_zeroberto.ZeroBERToModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "                                            hypothesis_template=hypothesis_template,\n",
    "                                            classes_list=classes_list,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/alealcoforado/.cache/huggingface/datasets/SetFit___json/SetFit--ag_news-bfd04d52440715fd/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 2/2 [00:00<00:00, 75.73it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_clusterer_fit_predict() missing 2 required positional arguments: 'leaf_size' and 'min_cluster_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _clusterer_fit_predict(\u001b[39m'\u001b[39;49m\u001b[39mhdbscan\u001b[39;49m\u001b[39m'\u001b[39;49m,embeds)\n",
      "\u001b[0;31mTypeError\u001b[0m: _clusterer_fit_predict() missing 2 required positional arguments: 'leaf_size' and 'min_cluster_size'"
     ]
    }
   ],
   "source": [
    "_clusterer_fit_predict('hdbscan',embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model2 = SentenceTransformer(\"all-roberta-large-v1\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = SentenceTransformer(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'].shuffle((42))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print((round(t2-t1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(tuple([torch.Tensor([3,4,5]),torch.Tensor([6,4,8]),torch.Tensor([10,2,3])]))\n",
    "(torch.Tensor([1])).device\n",
    "\n",
    "# list(tuple([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens=tuple([torch.Tensor([3]).to('cpu'),torch.Tensor([6]),torch.Tensor([10])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([3]).to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "listona = np.ones(10,int)\n",
    "# listona[tens.items()]\n",
    "# tens[0].item()\n",
    "[t.item() for t in list(tens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.Tensor([0.3079, 0.2746, 0.2581, 0.2576, 0.2571, 0.2569, 0.2553, 0.2553]), \n",
    "t2 = torch.tensor([0.3323, 0.2996, 0.2994, 0.2896, 0.2806, 0.2798, 0.2706, 0.2704, 0.2631,0.2594, 0.2590, 0.2588, 0.2557, 0.2537, 0.2532, 0.2528])\n",
    "\n",
    "# print((t1.shape))\n",
    "\n",
    "for t in t1:\n",
    "    print(len(t))\n",
    "\n",
    "print(torch.reshape(t1[0],(len(t1[0]),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(alo)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [['aleluia', 'a', 2, 1.00, 0.5],['bro', 'b', 1, 2.00, 0.5],['cavalo', 'c', 7, 4.00, 0.5],['cavalo', 'c', 3, 4.00, 0.45]]\n",
    "\n",
    "x,y,w,k,z = (zip(*lista))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista.sort(key= lambda x : x[3])\n",
    "print(lista)\n",
    "dict_it = {}\n",
    "unique_indices = []\n",
    "unique_data = []\n",
    "for sublist in lista:\n",
    "     if sublist[1] not in unique_indices:\n",
    "        unique_indices.append(sublist[1])\n",
    "        unique_data.append(sublist)\n",
    "\n",
    "        # print(sublist[1])\n",
    "        # print(dict_it)\n",
    "        this_sublist_dict['indice'] = sublist[3] \n",
    "        this_sublist_dict['text'] = sublist[0] \n",
    "        this_sublist_dict['label_train'] = sublist[1] \n",
    "        this_sublist_dict['true_label'] = sublist[2] \n",
    "\n",
    "\n",
    "        dict_it[sublist[1]] = this_sublist_dict\n",
    "print(dict_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('dbpedia_14')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][13,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard_indices = [tensor(2, device='cpu')]\n",
    "discard_indices = [torch.tensor(2),torch.tensor(3),torch.tensor(4)]\n",
    "# discard_indices = torch.tensor(discard_indices,dtype=torch.float64)\n",
    "probs = [[0.33,0.44,0.55,0.66,0.31,0.98], [0.11,0.44,0.55,0.66,0.31,0.98], [0.22,0.54,0.32,0.451,0.32,0.98], [0.11,0.44,0.55,0.66,0.31,0.98], [0.22,0.54,0.32,0.451,0.32,0.98], [0.11,0.44,0.55,0.66,0.31,0.98], [0.22,0.54,0.32,0.451,0.32,0.98], [0.11,0.44,0.55,0.66,0.31,0.98], [0.22,0.54,0.32,0.451,0.32,0.98]]\n",
    "probs = torch.Tensor(probs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(type(probs),(probs.shape))\n",
    "\n",
    "\n",
    "discard_indices = [tensor.item() for tensor in discard_indices]\n",
    "tensor = torch.ones(len(discard_indices),probs.shape[-1])\n",
    "\n",
    "probs[discard_indices] = -1*tensor\n",
    "print((discard_indices))\n",
    "# print(probs)\n",
    "# v,i = torch.topk(probs,k=1,dim=0)\n",
    "# top_prob,index = v.T,i.T\n",
    "# top_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[discard_indices] = -1.0*torch.ones(len(discard_indices), probs.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'eval':{'weighted':{'accuracy':2}}}\n",
    "print(dict[list(dict.keys())[0]]['weighted']['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_ds = load_dataset('SetFit/CR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_ds['train']['label_text'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = [[0,1,2],[3,5,4],[10,13,15],[6,5,4]]    \n",
    "df_test = pd.DataFrame(test_matrix)\n",
    "idxmaxes = df_test.apply(lambda row: row.idxmax(), axis=1).to_list()\n",
    "maxes = df_test.apply(lambda row: row.max(), axis=1).to_list()\n",
    "\n",
    "maxes,idxmaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_max = [np.argmax(lista) for lista in (np.array(test_matrix))]\n",
    "valores_max = [np.max(lista) for lista in (np.array(test_matrix))]\n",
    "\n",
    "valores_max,indices_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = ([np.ndarray([1,2,3]),np.ndarray([4,5,6]),np.ndarray([4,5,6]),np.ndarray([4,5,6])])\n",
    "new = []\n",
    "for array in test_arrays:\n",
    "    new.append(torch.Tensor(array))\n",
    "print(torch.stack(new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensors  = [torch.Tensor([0,1,2]),torch.Tensor([3,5,4]),torch.Tensor([10,13,15]),torch.Tensor([6,5,4])]    \n",
    "\n",
    "# torch.stack(test_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed in embeddings:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = dataset['train'].select(range(0,100))\n",
    "st = SentenceTransformer('all-roberta-large-v1')\n",
    "embeddings = st.encode(train_dataset['text'])\n",
    "clusterer = hdbscan.HDBSCAN(leaf_size=10, min_cluster_size=2)\n",
    "clusters = clusterer.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs = np.array([(np.arange(0,100,1) * np.arange(100,0,-1) / 1000), (np.arange(0,100,1) * np.arange(100,0,-1) / 2000),\n",
    "         (np.arange(0,100,1) * np.arange(100,0,-1) / 2000),(np.arange(0,100,1) * 2.7*np.arange(100,0,-1) / 2000)]).reshape(100,4)\n",
    "labels = dataset['train'].select(range(0,100))['label']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_intraclass_clustering_data(text_list,probabilities,true_labels,embeddings,n,clusterer='hdbscan',leaf_size=10,min_cluster_size=10):\n",
    "    df_probs = pd.DataFrame(probabilities)\n",
    "    label_results = df_probs.apply(lambda row: row.idxmax(), axis=1).to_list()\n",
    "    prob_results = df_probs.apply(lambda row: row.max(), axis=1).to_list()\n",
    "\n",
    "    unique_labels = list(set(label_results))\n",
    "    unique_labels.sort()\n",
    "    print(unique_labels)\n",
    "    all_labels_selected_data = []\n",
    "    for label in unique_labels:\n",
    "        this_label_selected_data = []\n",
    "        this_label_indexes = [i for i in range(len(label_results)) if label_results[i] == label]\n",
    "        # print(this_label_indexes)\n",
    "        this_label_text_list =  [text_list[i] for i in this_label_indexes]\n",
    "        this_label_embeddings =  [embeddings[i] for i in this_label_indexes]\n",
    "        this_label_probs =  [prob_results[i] for i in this_label_indexes]\n",
    "        this_label_true_labels = [true_labels[i] for i in this_label_indexes]\n",
    "        this_label_label_results = [label_results[i] for i in this_label_indexes]\n",
    "\n",
    "\n",
    "        print(\"Clustering class {}.\".format(label))\n",
    "        # logger.info(\"Clustering class {}.\")\n",
    "\n",
    "        this_label_clusters = _clusterer_fit_predict(clusterer, this_label_embeddings, leaf_size, min_cluster_size) \n",
    "        # print(len(this_label_clusters),len(this_label_indexes),len(this_label_text_list),len(this_label_embeddings),len(this_label_probs))\n",
    "        unique_clusters = list(set(this_label_clusters))\n",
    "        unique_clusters.sort()\n",
    "        # print(unique_clusters)\n",
    "        all_clusters_sorted_lists = []\n",
    "\n",
    "        # organize by sorting sorting and zipping lists, 1 list for each cluster found\n",
    "        for cluster in unique_clusters:\n",
    "            this_cluster_indexes = [i for i in range(len(this_label_clusters)) if this_label_clusters[i] == cluster]\n",
    "            this_cluster_probs =  [this_label_probs[i] for i in this_cluster_indexes]\n",
    "            this_cluster_texts = [this_label_text_list[i] for i in this_cluster_indexes]\n",
    "            this_cluster_true_labels = [this_label_true_labels[i] for i in this_cluster_indexes]\n",
    "            this_cluster_label_results = [this_label_label_results[i] for i in this_cluster_indexes]\n",
    "            zipped_lists = (list(zip(this_cluster_probs,this_cluster_indexes,this_cluster_true_labels,this_cluster_label_results,this_cluster_texts)))\n",
    "            zipped_lists.sort(reverse=True)\n",
    "            # print(zipped_lists)\n",
    "\n",
    "            all_clusters_sorted_lists.append(zipped_lists)\n",
    "        # selects data iteratively, 1 from each cluster from biggest to smallest cluster, \n",
    "        # following highest probability order inside each cluster\n",
    "        while len(this_label_selected_data) < n:\n",
    "            for sorted_list in all_clusters_sorted_lists:\n",
    "                # print((all_clusters_sorted_lists))\n",
    "                if len(sorted_list) > 0:\n",
    "                    # print(sorted_list)\n",
    "                    selected_element = sorted_list[0]\n",
    "                    print(label,selected_element)\n",
    "                    this_label_selected_data.append(selected_element)\n",
    "                    sorted_list.pop(0)\n",
    "                    # print(sorted_list)\n",
    "                    if len(this_label_selected_data) == n:\n",
    "                        break\n",
    "\n",
    "                # print(all_clusters_sorted_lists)\n",
    "            if len(all_clusters_sorted_lists) == 0 or all_clusters_sorted_lists == [[]]:\n",
    "                # print(label)\n",
    "                print(\"Not enough data to sample for label {label}: {n} samples expected, but only got {this_label_n}\".format(label=label,n=n,this_label_n=len(this_label_selected_data)))\n",
    "                # logger.info(\"Not enough data to sample for label {label}: {n} samples expected, but only got {this_label_n}\".format(label=label,n=n,this_label_n=len(this_label_selected_data)))\n",
    "                break\n",
    "        all_labels_selected_data.append(this_label_selected_data)\n",
    "\n",
    "    flat_selected_data = [item for sublist in all_labels_selected_data for item in sublist]\n",
    "\n",
    "    probs,train_indices,true_labels,train_labels,texts = zip(*flat_selected_data)\n",
    "\n",
    "\n",
    "    # x_train = [text_list[i] for i in train_indices]\n",
    "    # y_train = [true_labels[i] for i in train_indices]\n",
    "    # labels_train = [label_results[i] for i in train_indices]\n",
    "    # print(x_train,y_train,labels_train)\n",
    "\n",
    "    x_train = texts\n",
    "    y_train = true_labels\n",
    "    labels_train = train_labels \n",
    "    print(x_train,y_train,labels_train)\n",
    "                                 \n",
    "    return x_train, y_train, labels_train\n",
    "\n",
    "def _clusterer_fit_predict(clusterer,embeddings,leaf_size,min_cluster_size):\n",
    "    if clusterer=='hdbscan':\n",
    "        clusterer = hdbscan.HDBSCAN(leaf_size=leaf_size, min_cluster_size=min_cluster_size)\n",
    "    # print(len(embeddings))\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "    # logger.info(\"Found {} clusters.\".format(len(list(set(clusters)))))\n",
    "    print(\"Found {} clusters.\".format(len(list(set(clusters)))))\n",
    "    return clusters\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, labels_train = _get_intraclass_clustering_data(train_dataset['text'],probs,labels,embeddings,8,min_cluster_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_zeroberto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
